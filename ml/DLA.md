##PCA
  * 最大方差理论
  * 最小均方理论
  * http://blog.csdn.net/jirongzi_cs2011/article/details/9499011
    

##LDA与PCA的比较
  * PCA无需样本标签, 属于无监督降维; LDA需要样本标签, 属于有监督降维. 二者均是寻找一定的特征向量w来降维的, 其中, LDA抓住样本的判别特征
    PCA侧重描述特征. PCA选择样本点投影具有最大方差的方向, LDA选择分类性能最好的方向.
  * PCA降维是直接和特征维度相关的，比如原始数据是d维的, 那么PCA后, 可以任意选取1维， 2维,  一直到d维都行. LDA降维是直接和个数C相关的
    , 与数据本身的维度无关. 比如原始数据是d维的, 一共有c个类别, 那么LDA降维后，一般就是1维， 2维 ....c-1维进行选择。要求降维后的特征
    大于c-1维则LDA不能试用. 
  * PCA投影的坐标系都是正交的, 而LDA根据类别的标注关注分类能力, 因此不能保证投影到的坐标系是正交的(一般都不正交)
  * LDA,PCA都不适合对非高斯分布样本进行降维。如果数据是高斯分布的话，那么我们只需要利用这批数据的一阶和二阶统计量就能对数据分布进行描述（即均值和方        差）。而PCA在处理数据时仅需要用到最多二阶的统计量，因此如果数据需要三阶或是更高阶的统计量对其分布进行描述的话，，LDA,PCA就没办法对该数据进行描述和处     理了
  * http://www.cnblogs.com/engineerLF/p/5393119.html

##LDA的局限
* LDA仅仅考虑了欧式结构(算类内矩阵和类间矩阵)，所以它不能发现数据在高维非高斯分布的非线性结构.
  解决方法: manifold learning 分析高维数据依靠观测到空间的局部，也就是用这个点周围的点去描述这个点。
* LDA认为所有点降维后的贡献是一样的
  解决方法：增加边界点的权重
* 矩阵会有奇异
  转化为解决一个标准的特征分解问题
 
##DLA
* 先对数据进行PCA降维
![](http://latex.codecogs.com/gif.latex?(x_1,x_2,.....)) -> ![](http://latex.codecogs.com/gif.latex?(y_1,y_2....))


