##PCA
  * 最大方差理论
  * 最小均方理论
  * http://blog.csdn.net/jirongzi_cs2011/article/details/9499011
    





##LDA与PCA的比较
  * PCA无需样本标签, 属于无监督降维; LDA需要样本标签, 属于有监督降维. 二者均是寻找一定的特征向量w来降维的, 其中, LDA抓住样本的判别特征
    PCA侧重描述特征. PCA选择样本点投影具有最大方差的方向, LDA选择分类性能最好的方向.
  * PCA降维是直接和特征维度相关的，比如原始数据是d维的, 那么PCA后, 可以任意选取1维， 2维,  一直到d维都行. LDA降维是直接和个数C相关的
    , 与数据本身的维度无关. 比如原始数据是d维的, 一共有c个类别, 那么LDA降维后，一般就是1维， 2维 ....c-1维进行选择。要求降维后的特征
    大于c-1维则LDA不能试用. 
   
  




##LDA的局限
* LDA仅仅考虑了欧式结构(算类内矩阵和类间矩阵)，所以它不能发现数据在高维非高斯分布的非线性结构.
  解决方法: manifold learning 分析高维数据依靠观测到空间的局部，也就是用这个点周围的点去描述这个点。
* LDA认为所有点降维后的贡献是一样的
  解决方法：增加边界点的权重
* 矩阵会有奇异

